---
title: "HarvardX: PH125.9X - Data Science: Capstone  \n   Soccer - Goal Difference Prediction"
author: "Wilson Tan"
date: "9 Dec 2020"
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

# Executive Summary

The objective of the project is to create a recommender system to predict matchday goal difference in the English Premier League using the football-data.co.uk dataset. The dataset is made up of data across 6,840 matches, with matchday goal difference ranging from -6 to 8 goals. These matches are played between 44 unique football clubs, stretching across 18 full seasons. 

90% of the dataset is set aside as "model" to train the model while the remaining is used as "validation" to evaluate the proposed models. The Root Mean Square Error (RMSE) is used to evaluate the algorithm performance. RMSE measures the differences between predicted values and true values. This is regarded as a standard way to measure the model's accuracy. The RMSE of predicted values $\hat{y}$ versus true values $y$, for $N$ observations (for HomeTeam $h$, AwayTeam $a$ and Form $f$) is given by: 

$$ 
RMSE = \sqrt{\frac{1}{N}\displaystyle\sum (\hat{y}_{h,a,f}-y_{h,a,f})^{2}} 
$$  
Considering $RMSE = 0$ would indicate a perfect fit to the data, a lower RMSE is generally desired over a higher one. The best performing model has registered a RMSE of 1.552, representing a substantial improvement from the RMSE of 1.702 based on the Naive Baseline Model. The major factors considered in this model are namely: 

+ Difference in the Strength between the Teams: Matchup (Hometeam, Awayteam) bias and 
+ Difference in the Performance between the Teams leading up to the matchday: Form (Normalised Goal Difference, Normalised Point Difference) bias 

Regularization is further applied to the model to mitigate the risk of overfitting. Regularization serves to penalize on matchups with limited occurrences. This model is applied on the validation set to achieve a RMSE of 1.550. With the English Premier League regarded as the most competitive and unpredictable league in the world, a RMSE which falls close to ~10% of the range of the target value should be considered an acceptable error. Nonetheless, the model still has room for further improvement. For instance, game statistics (i.e. shots on target, possession, corners, etc), the formation deployed by the team and the ability of individual players (especially goal scoring prowess, injuries) can be included in future models. Unfortunately, due to the limitations on the data available, these models cannot be validated. 

\newpage
# Introduction

Soccer is a physical sport played between 2 teams of 11 players, with a designated goalkeeper and 10 outfield players. The match is often played on a large grass field with each team attempting to score a goal by placing the ball across the line into the opposing team's goal. The team with the highest number of goals scored in a game wins the match. 

Some of the acronyms used in this prediction model will be defined in the following table: 

\begin{center} Table 2.1 Definition of Acronyms \end{center}
Acronym         | Definition
----------------|------------------------------------
FTHG            | Full Time Home Goals Scored
FTAG            | Full Time Away Goals Scored
FTR             | Full Time Results (Final Scoreline)
HTFormPtsStr    | Home Team Last 5 Home Games Results
ATFormPtsStr    | Away Team Last 5 Away Games Results
HTGD            | Home Team Normalized Goal Difference
ATGD            | Away Team Normalized Goal Difference
GD              | Normalized Goal Difference (HTGD - ATGD)
goal_diff       | Matchday Goal Difference (FTHG - FTAG)

Data processing is carried out in Section 2.3 to organize continuous data like the normalized goal difference and normalized point difference into discrete categories: 

\begin{center} Table 2.2 Normalised Goal Difference Category \end{center}
Abb | Normalised Goal Diff Category | Definition
----|-------------------------------|-------------------------
G1  | 1 - Overwhelming Disadvantage | GD <= -3.5
G2  | 2 - Huge Disadvantage         | -3.5 < GD <= -2.5
G3  | 3 - Disadvantage              | -2.5 < GD <= -1.5
G4  | 4 - Slight Disadvantage       | -1.5 < GD <= -0.5
G5  | 5 - Neutral                   | -0.5 < GD < -0.5
G6  | 6 - Slight Advantage          | 0.5 <= GD < 1.5
G7  | 7 - Advantage                 | 1.5 <= GD < 2.5
G8  | 8 - Huge Advantage            | 2.5 <= GD < 3.5
G9  | 9 - Overwhelming Advantage    | GD >= 3.5

\begin{center} Table 2.3 Normalised Point Difference Category \end{center}
Abb | Normalised Point Diff Category | Definition
----|--------------------------------|------------------------------
P1  | 1 - Worst Form                 | PD <= -2.5
P2  | 2 - Worse Form                 | -2.5 < PD <= -1.5 
P3  | 3 - Poor Form                  | -1.5 < PD <= -0.5
P4  | 4 - Neutral                    | -0.5 < PD < 0.5
P5  | 5 - Good Form                  | 0.5 <= PD < 1.5
P6  | 6 - Better Form                | 1.5 <= PD < 2.5
P7  | 7 - Best Form                  | PD >= 2.5

As there is a strong correlation between normalized goal difference and normalized point difference, the two categories are combined to produce the GDPD Category: 

\begin{center} Table 2.4 Form Difference (GDPD) Category \end{center}
GDPD| Goal Diff Category            | Point Diff Category 
----|-------------------------------|--------------------
G1P1| 1 - Overwhelming Disadvantage | 1 - Worst Form
G1P2| 1 - Overwhelming Disadvantage | 2 - Worse Form
    | ...
G5P4| 5 - Neutral                   | 4 - Neutral
    | ...
G9P6| 9 - Overwhelming Advantage    | 6 - Better Form
G9P7| 9 - Overwhelming Advantage    | 7 - Best Form

# Preparation  

This section elaborates on the steps taken from installing libraries through data processing to train-test split.

## Prerequisites

The libraries required in this modeling are as follow: 

```{r, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
# Install the required libraries if not already present
if(!require(tidyverse)) 
  install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) 
  install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) 
  install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(recosystem)) 
  install.packages("recosystem", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) 
  install.packages("kableExtra", repos = "http://cran.us.r-project.org")
```
```{r, echo=TRUE, message=FALSE, warning=FALSE, eval=TRUE}
# Load installed libraries
library(tidyverse)
library(caret)
library(data.table)
library(recosystem)
library(kableExtra)


```

The operating system used in this modeling are as follow: 
```{r, echo=FALSE, include=TRUE, message=FALSE, warning=FALSE}
version
```

## Access to Data

The dataset can be downloaded from https://www.kaggle.com/saife245/english-premier-league. Individual datasets are available on https://www.football-data.co.uk/englandm.php but additional processing is required to produce the data required to evaluate the difference in performance between the teams (i.e. HTGD, ATGD, HTFormPtsStr, ATFormPtsStr). 

```{r, echo=FALSE, include=FALSE}
# Access to soccer dataset on local drive
# Credits to SAIF UDDIN (Source: football-data.co.uk)
soccer <- read.csv("soccer.csv") 
```


## Data Processing

```{r, echo=FALSE, include=FALSE}
dim(soccer)

```

A quick overview on the dimension of the dataset indicates that there 40 columns within the dataset. For simplicity, only selected features are extracted to be presented in the following table:  


**Unprocesssed soccer dataset**

```{r unprocessed_data, echo=FALSE}
tail(soccer) %>%
  select("MatchId", "Date", "HomeTeam", "AwayTeam", "FTHG", "FTAG", "FTR") %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)


tail(soccer) %>%
  select("HTFormPtsStr", "ATFormPtsStr", "HTGD", "ATGD") %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)
```

From the above table, it is notable that some of the features can be further processed. These include: 

1. Extract the ```Year``` and ```Month``` from the ```date``` feature;  
2. Creating the matchday goal difference, ```goal_diff```, by comparing the difference between the ```FTHG``` and ```FTAG``` features to represent the goal difference for the specific match. This is the target value that the model will be predicting;  
3. Creating ```GD``` by comparing the difference between the ```HTGD``` and ```ATGD``` features before converting this continuous feature to discrete categories. The range of these categories is determined after a thorough analysis elaborated in Section 4.3. This conversion to discrete categories is necessary to avoid a situation where there are too many unique values which could not be matched between the model and validation datasets *(Note that the ```HTGD``` and ```ATGD``` features are normalized to per match basis in the original soccer dataset)*;  
4. Creating ```PD``` by first assigning points to the ```HomeTeamPtsStr``` and ```AwayTeamPtsStr``` with:
+ 3 points representing a win (W) 
+ 1 point for draw (D); 
+ 0 point for loss (L)

These points are normalized to per match basis before being compared between the two teams. For the same reason as ```GD```, this continuous feature is then converted to discrete categories, as explained in Section 4.4;
5. Since scoring goals, limiting goals conceded and winning games have a high correlation, combining ```GD``` and ```PD``` features categories to form up ```GDPD``` category would be rather intuitive. 

```{r, echo=FALSE, include=FALSE}
soccer <- 
  soccer %>%
  mutate(Match = paste(HomeTeam, AwayTeam, sep = "-", collapse = NULL),
         # assigning indices to the HomeTeam and AwayTeam
         HomeTeamId = group_indices(soccer, .dots="HomeTeam"),
         AwayTeamId = group_indices(soccer, .dots="AwayTeam"),
         goal = FTHG + FTAG,
         # goal difference of the specific match (target value that the model will be predicting)
         goal_diff = FTHG - FTAG,
         # assigning points to HTFormPtsStr and ATFormPtsStr (3 points for W, 1 point for D, 0 point for L)
         Form_HTMatch = str_count(HTFormPtsStr,"W") + str_count(HTFormPtsStr, "D") + str_count(HTFormPtsStr, "L"),
         Form_ATMatch = str_count(HTFormPtsStr,"W") + str_count(HTFormPtsStr, "D") + str_count(HTFormPtsStr, "L"),
         Form_HTPts = 3 * str_count(HTFormPtsStr,"W") + 1 * str_count(HTFormPtsStr, "D"),
         Form_ATPts = 3 * str_count(ATFormPtsStr,"W") + 1 * str_count(ATFormPtsStr, "D"),
         Form_HT = round(if_else(Form_HTMatch <= 2, 0, Form_HTPts / Form_HTMatch), digits = 1),
         Form_AT = round(if_else(Form_ATMatch <= 2, 0, Form_ATPts / Form_ATMatch), digits = 1),
         # compare the form (in terms of points) between the two teams
         PD = round(Form_HT - Form_AT, 1),
         # converting the continuous form (points) to discrete categories
         PD_Category = case_when( 
           PD >= 2.5 ~ "7 - Best Form",
           PD < 2.5 & PD >= 1.5 ~ "6 - Better Form",
           PD < 1.5 & PD >= 0.5 ~ "5 - Good Form",
           PD < 0.5 & PD > -0.5 ~ "4 - Neutral",
           PD <= -0.5 & PD > -1.5 ~ "3 - Poor Form",
           PD <= -1.5 & PD > -2.5 ~ "2 - Worse Form",
           PD <= -2.5 ~ "1 - Worst Form"),
         PD_Cat_Abb = case_when( 
           PD >= 2.5 ~ "P7",
           PD < 2.5 & PD >= 1.5 ~ "P6",
           PD < 1.5 & PD >= 0.5 ~ "P5",
           PD < 0.5 & PD > -0.5 ~ "P4",
           PD <= -0.5 & PD >= -1.5 ~ "P3",
           PD <= -1.5 & PD >= -2.5 ~ "P2",
           PD <= -2.5 ~ "P1"),
         # compare the form (in terms of goals scored and conceded) between the two teams
         GD = HTGD - ATGD,
         # converting the continuous form (goals) to discrete categories
         GD_Category = case_when( 
           GD >= 3.5 ~ "9 - Overwhelming Advantage",
           GD < 3.5 & GD >= 2.5 ~ "8 - Huge Advantage",
           GD < 2.5 & GD >= 1.5 ~ "7 - Advantage",
           GD < 1.5 & GD >= 0.5 ~ "6 - Slight Advantage",
           GD < 0.5 & GD > -0.5 ~ "5 - Neutral",
           GD <= -0.5 & GD > -1.5 ~ "4 - Slight Disadvantage",
           GD <= -1.5 & GD > -2.5 ~ "3 - Disadvantage",
           GD <= -2.5 & GD > -3.5 ~ "2 - Huge Disadvantage",
           GD <= -3.5 ~ "1 - Overwhelming Disadvantage"),
         GD_Cat_Abb = case_when( 
           GD >= 3.5 ~ "G9",
           GD < 3.5 & GD >= 2.5 ~ "G8",
           GD < 2.5 & GD >= 1.5 ~ "G7",
           GD < 1.5 & GD >= 0.5 ~ "G6",
           GD < 0.5 & GD > -0.5 ~ "G5",
           GD <= -0.5 & GD > -1.5 ~ "G4",
           GD <= -1.5 & GD > -2.5 ~ "G3",
           GD <= -2.5 & GD > -3.5 ~ "G2",
           GD <= -3.5 ~ "G1"),
         GDPD = paste(GD_Cat_Abb, PD_Cat_Abb, sep=""),
         # extracting month and year from Date
         Month = month(Date),
         Year = year(Date)) %>%
  select(MatchId, Match, 
         Date, Year, Month, 
         HomeTeamId, HomeTeam, AwayTeamId, AwayTeam, 
         FTR, FTHG, FTAG, goal, goal_diff, 
         GD, GD_Category, GD_Cat_Abb, PD, PD_Category, PD_Cat_Abb, GDPD)
```


Following the data processing, only features with significance are selected to remain in the table for subsequent analysis and modeling. 

**Processed soccer dataset**
```{r processed_data, echo=FALSE}
tail(soccer) %>%
  select("MatchId", "Date",  "Year",  "Month", "HomeTeam", "HomeTeamId", "AwayTeam", "AwayTeamId") %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)


tail(soccer) %>%
  select("FTR", "FTHG", "FTAG", "goal", "goal_diff", "GD_Category", "PD_Category", "GDPD") %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)
```

## Model-Validation Split

In order to evaluate the performance of the model, the soccer dataset is split into 2 subsets, "model" and "validation" while making sure the HomeTeam, AwayTeam and GDPD. Algorithm development will be carried out on the "model" subset while "validation" subset will be used to test the final algorithm. 

```{r, echo=FALSE, include=FALSE}
# Create validation set which will be set to 10% of soccer data
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = soccer$goal, times = 1, p = 0.1, list = FALSE)
model <- soccer[-test_index,]
temp <- soccer[test_index,]

# Make sure HomeTeam, AwayTeam and GDPD in validation set are also in model set
validation <- temp %>% 
  semi_join(model, by = "HomeTeam") %>%
  semi_join(model, by = "AwayTeam") %>%
  semi_join(model, by = "GDPD")

# Add rows removed from validation set back into model set
removed <- anti_join(temp, validation)
model <- rbind(model, removed)

```

## Train-Test Split

The model set is split further, with 90% of the data allocated to the "train" set and the remaining data allocated to the "test" set. The train set is a sample of data used to fit the model while the test set will used to provide an unbiased evaluation of a model fit on the train dataset while tuning the model parameters.

```{r, echo=FALSE, include=FALSE}
# Further splitting the model set to train and test sets
set.seed(1, sample.kind="Rounding")

# Create test set which will be set to 10% of model set
test_index <- createDataPartition(y = model$goal, times = 1, p = 0.1, list = FALSE)
train <- model[-test_index,]
temp <- model[test_index,]

# Make sure HomeTeam, AwayTeam and GDPD in test set are also in train set
test <- temp %>% 
  semi_join(train, by = "HomeTeam") %>%
  semi_join(train, by = "AwayTeam") %>%
  semi_join(train, by = "GDPD")

# Add rows removed from test set back into train set
removed <- anti_join(temp, test)
train <- rbind(train, removed)

# Remove unnecessary datasets
rm(removed, temp, test_index) 

```

# Data Analysis

This section elaborates on the steps taken to identify notable trends and correlations between the rating and the features. 

## Number of Matches based on Matchday Goal Difference

```{r distribution_goal_diff, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4}
# Distribution of goal_diff by match
model %>%
  ggplot(aes(goal_diff)) +
  geom_histogram(bins = 12, color = "orange") + 
  labs(x = "Matchday Goal Difference", 
       y = "Number of Matches", 
       title = "Histogram - Matches distribution through Matchday Goal Difference")
```

The histogram is a unimodal distribution with a single peak at 0 matchday goal difference. Another point to note is that the distribution is skewed left, meaning the majority of the observations above 0, with only a handful of observations being being negative. 

## Matchday Goal Difference in Typical Matchup

```{r distribution_heatmap_matchup, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6}
# Distribution of goal difference scored in Typical Matchups (HomeTeam VS AwayTeam)
p <- model %>%
  group_by(Match) %>%
  summarize(HomeTeam = HomeTeam,
            AwayTeam = AwayTeam,
            goal_diff = mean(goal_diff))


p <- distinct(p)

p %>% ggplot(aes(HomeTeam, 
                 AwayTeam, 
                 fill = goal_diff)) +
  geom_tile() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3)) +
  scale_fill_gradient2(low = "red",
                       high = "green", 
                       midpoint = 0) +
  labs(title = "Heat Map - Avg. No. of Goals Scored in Typical Matchups")
```

From the heat map distribution, it can be observed that certain matchups tend to register a higher goal difference than the others. There is also strong evidence that when matches are played on the home ground of the "Big Six" (i.e. Arsenal, Chelsea, Liverpool, Man City, Man United or Tottenham), the matchday goal difference tend to end up positive, represented by the green tiles observed in these "Big Six" vertical column. In other words, these "Big Six" teams tend to score more goals than they concede in their respective home ground. The major contributing factor here is the difference in strengths between these "Big Six" teams and the AwayTeam. To further illustrate this difference in strengths, it is necessary to take a closer look at goals scored and goals conceded by the individual team.

```{r distribution_goals_scoredVSconceded_home, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6}
# Distribution of goals scored and goals conceded based on HomeTeam

model %>% 
  group_by(HomeTeam) %>%
  summarize(FTHG = mean(FTHG),
            FTAG = mean(FTAG)) %>%
  ggplot(aes(FTAG, 
             FTHG, 
             label = HomeTeam,
             color = HomeTeam)) +
  geom_label(size=2.5, label.padding = unit(0.1, "lines")) +
  theme(legend.position = "none") +
  labs(x = "Number of Goals Conceded in Home Games",
       y = "Number of Goals Scored in Home Games",
       title = "Distribution - Avg Goals Scored VS Avg Goals Conceded by Home Team")
```

As expected, the "Big Six" teams are concentrated at the top left of the distribution, implying that these teams tend to score more and concede less goals in their home ground. From this distribution, it can also be inferred that other than a few outliers, there is an inverse relationship between goals scored and goals conceded. As explained, stronger teams normallyto score more and concede less when playing in their home ground whereas weaker teams not only find it more difficult to score but also tend to concede more. A similar trend would be expected if distribution is plotted from the perspective of the Away Team. 

```{r distribution_goals_scoredVSconceded_away, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6}
# Distribution of goals scored and goals conceded based on AwayTeam

model %>% 
  group_by(AwayTeam) %>%
  summarize(FTAG = mean(FTAG),
            FTHG = mean(FTHG)) %>%
  ggplot(aes(FTHG, 
             FTAG, 
             label = AwayTeam,
             color = AwayTeam)) +
  geom_label(size=2.5, label.padding = unit(0.1, "lines")) +
  theme(legend.position = "none") +
  labs(x = "Number of Goals Conceded in Away Games",
       y = "Number of Goals Scored in Away Games",
       title = "Distribution - Avg Goals Scored VS Avg Goals Conceded by Away Team")
```

Similarly, the "Big Six" teams are concentrated at the top left of the distribution. One difference between the Home Team distribution and Away Team distribution is that in the Home Team distribution, majority of the teams are centered around 1.0 to 1.5 goals scored and 1.0 to 1.5 goals conceded whereas in the Away Team distribution, majority of the teams are centered around 0.75 to 1.25 goals scored and 1.4 to 1.8 goals conceded. This indicates that for majority teams outside of the "Big Six" playing at home, there is barely anything to separate goals scored from goals conceded. However, in away games, these team tend to concede more than scoring.

## Matchday Goal Difference versus Form (Normalised Goal Difference)

```{r distribution_goaldiff_VS_normalisedGD, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4}
# Distribution of goal_diff by HomeTeamGD and AwayTeamGD

model %>%
  ggplot(aes(GD, 
             goal_diff,
             fill = count)) +
  stat_bin2d(aes(fill = after_stat(count)), 
             binwidth = c(0.5,1)) +
  scale_fill_gradient(low = "grey",
                      high = "blue") + 
  theme(title = element_text(size = 8)) +
  labs(x = "Form - Normalised Goal Difference (Leading to Game)",
       y = "Matchday Goal Difference",
       title = "Heat Map - Matchday Goal Difference VS Form (Normalised Goal Difference)")
```

Normalized Goal Difference (GD) is determined by taking the difference between the normalized goal differences registered by the HomeTeam (HTGD) and the AwayTeam (ATGD). These figures, normalized to per match basis, are calculated from the Number of Goals Scored and Conceded by the respective team throughout the season, before being normalized to per match basis. From the heat map, GD appears to have a positive correlation with the the matchday goal difference, with a huge concentration of matches registering matchday goal difference between -2 and 2 with GD between -1.5 and 1.5. The positive correlation suggests that a matchup with higher GD would expect a higher matchday goal difference and vice versa. 

Since the GD involves continuous data, this could result in very distinct values of GD in different matchups. As such, there may be a significant number of GD values calculated in the model dataset not be repeated (distinct) in the validation dataset and vice versa. In order to avoid such a scenario, the proposal is to convert this continuous GD value to a discrete categorical class to avoid any mismatch in subsequent modeling. Observations from this heat map is used to determine the range of GD values in each category. For instance, for matchups where there is a GD of less than -3.5, the matchday goal difference is likely to be negative, whereas for matchups where there is a GD of more than 3.5, the matchday goal difference is likely to be positive. Categories between these two limits are specified with a span of 1. These categories are provided in Table 2.2. 

*Note: These considerations have been taken into account in Section 3.3 Data Processing .* 

Further analyses on the distribution of these categories are carried out to substantiate the trend explained. As presented in the following charts, the observations are aligned with the initial understanding of the relationship between GD and matchday goal difference. 

```{r distribution_goaldiff_VS_GDCategory, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3}
# Distribution of goal_diff by GD Category

model %>% 
  group_by(GD_Category) %>%
  summarise(count = n()) %>%
  ggplot(aes(count, GD_Category)) +
  geom_col(color = "blue") + 
  theme(title = element_text(size = 8)) +
  labs(x = "Number of Matches",
       y = "Form - Normalised Goal Difference Category",
       title = "Distribution - Form (Normalised GD Category) VS Number of Matches")

```

```{r boxplot_goaldiff_VS_GDCategory, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5.5}
# Boxplot of goal_diff by GD Category
model %>%
  ggplot(aes(GD_Category, goal_diff)) + 
  geom_boxplot(color = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        title = element_text(size = 8)) +
  labs(x = "Form - Normalised Goal Difference Category",
       y = "Matchday Goal Difference",
       title = "Box Plot - Matchday Goal Difference VS Form (Normalised GD Category)")
```

## Matchday Goal Difference versus Form (Normalised Point Difference)

```{r distribution_goaldiff_VS_normalisedPD, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4}
# Distribution of goal_diff by HomeTeamPD and AwayTeamPD

model %>%
  ggplot(aes(PD, 
             goal_diff,
             fill = count)) +
  stat_bin2d(aes(fill = after_stat(count)), 
             binwidth = c(1/2, 1)) +
  scale_fill_gradient(low = "grey",
                      high = "orange") +
  theme(title = element_text(size = 8)) +
  labs(x = "Form - Normalised Point Difference (Leading to Game)",
       y = "Matchday Goal Difference",
       title = "Heat Map - Matchday Goal Difference VS Form (Normalised Point Difference)")
```

Normalized Point Difference (PD) is determined by taking the difference between the normalized point differences registered by the HomeTeam and the AwayTeam. These figures are calculated by first assigning 3 points to a win and 1 point to a draw in the last 5 games, before being normalized to per match basis. From the heat map, PD appears to have a positive correlation with the the matchday goal difference, with a huge concentration of matches registering matchday goal difference between -2 and 2 with PD between -1 and 1. Although not as apparent as GD, the positive correlation suggests that a matchup with higher PD would expect a higher matchday goal difference and vice versa. 

A similar approach is adopted to convert the continuous PD values to discrete categorical class to avoid mismatch between the model and validation datasets. These categories are provided in Table 2.3.  

*Note: These considerations have been taken into account in Section 3.3 Data Processing .* 

Further analyses on the distribution of these categories are carried out to substantiate the trend explained. As presented in the following charts, the observations are aligned with the initial understanding of the relationship between PD and matchday goal difference. . 

```{r distribution_goaldiff_VS_PDCategory, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3}
# Distribution of goal_diff by PD Category

model %>% 
  group_by(PD_Category) %>%
  summarise(count = n()) %>%
  ggplot(aes(count, PD_Category)) +
  geom_col(color = "orange") + 
  theme(title = element_text(size = 8)) +
  labs(x = "Number of Matches",
       y = "Form - Normalised Point Difference Category",
       title = "Distribution - Form (Normalised Point Difference Category) VS Number of Matches")

```

```{r boxplot_goaldiff_VS_PDCategory, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5.5}
# Boxplot of goal_diff by PD Category
model %>%
  ggplot(aes(PD_Category, goal_diff)) + 
  geom_boxplot(color = "orange") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        title = element_text(size = 8)) +
  labs(x = "Form - Normalised Point Difference Category",
       y = "Matchday Goal Difference",
       title = "Box Plot - Matchday Goal Difference VS Form (Normalised Point Difference Category)")
```

## Matchday Goal Difference versus Form (Combined Normalised Point and Goal Difference)

Scoring more goals than conceding will win games, resulting in more points. Under this logic, combining GD and PD categories would be intuitive and is expected to produce more accurate predictions for the matchday goal difference than the individual categories. A facet grid is employed to allow multiple axes (GD, DD, matchday goal difference) to be reflected on the same plot for a more comprehensive study of the relationship between GD and PD. Number of matches is represented by the size of the point. 

```{r facetgrid_goaldiff_VS_normalisedPD&GD, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6}

# Facet Grid - goal_diff VS Form (GD + PD)
p <- model %>% 
  ggplot(aes(PD_Cat_Abb, goal_diff)) + 
  geom_count()

p + facet_grid(. ~ GD_Cat_Abb) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        title = element_text(size = 8),
        legend.position = "none") +
  labs(x = "Form - Goal Difference + Point Difference Category",
       y = "Matchday Goal Difference",
       title = "Distribution - Matchday Goal Difference VS Form (Goal Difference + Point Difference Category)")
```

As expected, moving across the plot, there is a shift in the number of matches towards the right, both in GD and PD Category. A HomeTeam playing with a better GD and PD is expected score more and concede less goals. At the same time, it can be inferred that between GD and PD, GD seemed to have a stronger influence on the matchday goal difference. This explains for the GD Category being placed in front of the combined GDPD category to reflect its importance. The following plots will be better representations of this trend. 


```{r distribution_goaldiff_VS_PDGD, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.5}

# Distribution - Average goal_diff VS Form (GDPD)
model %>%
  group_by(GDPD) %>%
  summarize(goal_diff = mean(goal_diff),
            count = n()) %>%
  ggplot(aes(GDPD, 
             goal_diff, 
             fill = count)) + 
  geom_col() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        title = element_text(size = 8)) +
  labs(x = "Form - Combined Goal Difference + Point Difference Category",
       y = "Average Matchday Goal Difference",
       title = "Distribution - Average Matchday Goal Difference VS Form (Combined GDPD Category)")
```


```{r boxplot_goaldiff_VS_PDGD, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.5}
# Box Plot - goal_diff VS Form (GDPD)
model %>% 
  ggplot(aes(GDPD, goal_diff, fill = GDPD)) +
  geom_boxplot(alpha = 0.3) + 
  stat_summary(fun = mean, geom = "point", shape = 23, size = 2) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        title = element_text(size = 8),
        legend.position = "None") +
  labs(x = "Form - Combined Goal Difference + Point Difference Category",
       y = "Matchday Goal Difference",
       title = "Box Plot - Matchday Goal Difference VS Form (Combined GDPD Category)")
```

From both plots, it can be observed that both the mean and median matchday goal difference increases moving across the plots. It is also worth noting that some categories happen less compared to others and there are even cases where the category did not occur in the model dataset. 

## Matchday Goal Difference versus Month & Year

```{r distribution_goaldiff_VS_Month, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.5}
# Distribution of goal by Month
p <- model %>% 
  mutate(Month = case_when( 
    Month == 8 ~ "01 - Aug",
    Month == 9 ~ "02 - Sep",
    Month == 10 ~ "03 - Oct",
    Month == 11 ~ "04 - Nov",
    Month == 12 ~ "05 - Dec",
    Month == 1 ~ "06 - Jan",
    Month == 2 ~ "07 - Feb",
    Month == 3 ~ "08 - Mar",
    Month == 4 ~ "09 - Apr",
    Month == 5 ~ "10 - May")) 

p %>% group_by(Month) %>%
  summarize(goal_diff = mean(goal_diff)) %>%
  ggplot(aes(Month, goal_diff)) +
  geom_col(color = "green") +
  labs(x = "Months (Season starts in Aug)",
       y = "Average Matchday Goal Difference",
       title = "Distribution - Avg. Matchday Goal Difference VS Month")

```

From the distribution, it can be observed that as the season progresses, there is a increase in the average matchday goal difference. This observation could be a result of teams gaining goal scoring prowess as players become more familiar with the team's formation and playing style.  

```{r distribution_goaldiff_VS_Year, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.5}
# Distribution of goal by Year
model %>% 
  group_by(Year) %>%
  summarize(goal_diff = mean(goal_diff)) %>%
  ggplot(aes(Year, goal_diff)) +
  geom_col(color = "green") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3)) +
  labs(x = "Years",
       y = "Average Matchday Goal Difference",
       title = "Distribution - Avg. Matchday Goal Difference VS Year")
```


Unlike the month feature, the significance of year remains inconclusive.

# Modeling Approach

## Naive-Baseline Model 

As the name suggests, the Naive-Baseline (Simple Average) Model will be used as the reference model for measuring the performance of subsequent models. The Naive-Baseline Model assumes that matchday goal difference across all matchups (HomeTeam and AwayTeam) will be the same. In this model, the mean value, which is approximated to be ~0.3826, will be used as the predicted rating for all reviews, regardless of movie or user. The formula of this Naive-Baseline Model can be represented by: 

$$Y_{h,a} = \hat{\mu} + \varepsilon_{h,a}$$
where $\hat{\mu}$ refers to the mean and $\varepsilon_{h,a}$ refers to the independent error sampled from the same distribution centered at 0. 

```{r echo=FALSE, include=FALSE}
# Compute the dataset's mean rating
mu <- mean(train$goal_diff)

# Test results based on simple prediction
rmse_baseline <- RMSE(test$goal_diff, mu)
accuracy_baseline <- mean(if_else(mu > 0.5, "H", "NH") == test$FTR)

# Check results
# Save prediction in data frame
rmse_results <- data_frame(Model = "[Test] Naive Baseline (Mean) Model", 
                           RMSE = rmse_baseline,
                           Accuracy = accuracy_baseline)

rmse_results %>% knitr::kable()

```
The RMSE of the Naive-Baseline Model on the ```test``` dataset is **1.702** with an accuracy of predicting home win at **54.1%**.

## Matchup Effect

As pointed out in the Data Analysis Section 4.2, both the HomeTeam and AwayTeam, collectively known as **Matchup**  has a strong influence in the matchday goal difference. Therefore, it would be sensible to include the HomeTeam effect, $b_h$, and the AwayTeam effect, $b_a$ to enhance the model. The resulting formula that represents the Matchup Effect Model is given by: 

$$Y_{h,a} = \hat{\mu} + b_h + b_a + \varepsilon_{h,a}$$
where:

+ $b_h$ refers to the HomeTeam effect or bias for HomeTeam $h$ and 
+ $b_a$ refers to the AwayTeam effect or bias for AwayTeam $a$

```{r echo=FALSE, include=FALSE}
## Matchup Effect Model ##

# Model taking into account the HomeTeam effect, b_h
b_h <- train %>%
  group_by(HomeTeam) %>%
  summarize(b_h = mean(goal_diff - mu))

# Model taking into account the AwayTeam effect, b_a
b_a <- train %>%
  left_join(b_h, by='HomeTeam') %>%
  group_by(AwayTeam) %>%
  summarize(b_a = mean(goal_diff - mu - b_h))

# predict all unknown goal_diff with mu, b_h and b_a
predicted_goaldiff <- test %>% 
  left_join(b_h, by='HomeTeam') %>%
  left_join(b_a, by='AwayTeam') %>%
  mutate(pred = mu + b_h + b_a) %>%
  pull(pred)

# calculate RMSE of Matchup effect
rmse_matchupeffect <- RMSE(test$goal_diff, predicted_goaldiff)

# calculate Accuracy of Matchup effect
accuracy_matchupeffect <- mean(if_else(predicted_goaldiff > 0.5, "H", "NH") == test$FTR)

# Test and save rmse results 
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="[Test] MatchUp Effect Model",  
                                     RMSE = rmse_matchupeffect,
                                     Accuracy = accuracy_matchupeffect))

# Consolidate results
rmse_results %>% knitr::kable()

```

The resulting RMSE of the Matchup Effect Model on the ```test``` dataset is **1.578** with an accuracy of predicting home win at **63.9%**. This represents an improvement in the RMSE of ~7.3% when compared with the Naive-Baseline Model. Comparing the RMSE with the actual matchday goal difference which spans from -6 to 14, this error is ~11.3%. 

## Matchup & Form Effect

Another feature that has a significant influence on the matchday goal difference is the **Form** difference of the HomeTeam and AwayTeam. For simplicity, this difference in Form is represented by discrete categories GDPD which is a combination of the Normalised Goal Difference and Normalised Point Difference. The reason for this combination is elaborated in the Data Analysis Section 4.5. Adding the Form effect, $b_f$, the resulting Matchup & Form Effect Model formula is given by: 

$$Y_{h,a,f} = \hat{\mu} + b_h + b_a + b_f + \varepsilon_{h,a}$$
where $b_f$ refers to the Form (GDPD) effect or bias $f$

```{r echo=FALSE, include=FALSE}
## Matchup & GDPD Model ##

# Model taking into account the GDPD effect, b_f
b_f <- train %>%
  left_join(b_h, by='HomeTeam') %>%
  left_join(b_a, by='AwayTeam') %>%
  group_by(GDPD) %>%
  summarize(b_f = mean(goal_diff - mu - b_h - b_a))

# predict all unknown goals with mu, b_h,  b_a and b_f
predicted_goaldiff <- test %>% 
  left_join(b_h, by='HomeTeam') %>%
  left_join(b_a, by='AwayTeam') %>%
  left_join(b_f, by="GDPD") %>%
  mutate(pred = mu + b_h + b_a + b_f) %>%
  pull(pred)

# calculate RMSE of Matchup + GDPD effect
rmse_matchupXGDPDeffect <- RMSE(test$goal_diff, predicted_goaldiff)

# calculate Accuracy of Matchup + GDPD effect
accuracy_matchupXGDPDeffect <- mean(if_else(predicted_goaldiff > 0.5, "H", "NH") == test$FTR)

# Test and save rmse results 
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="[Test] MatchUp & Form Effect Model",  
                                     RMSE = rmse_matchupXGDPDeffect, 
                                     Accuracy = accuracy_matchupXGDPDeffect))
# Consolidate results
rmse_results %>% knitr::kable()
```

The resulting RMSE of the Matchup & Form Effect Model on the ```test``` dataset is **1.573** with an accuracy of predicting home win at **64.8%**. This represents an improvement in the RMSE of ~7.6% when compared with the Naive-Baseline Model. Comparing the RMSE with the actual matchday goal difference which spans from -6 to 14, this error is ~11.2%. However, this is only a slight improvement from the Matchup Effect Model. 

## Matchup & Form & Month Effect

The last feature to exhibit a significant influence on the matchday goal difference is the **Month** of the matchday. The impact of the month on the matchday goal difference is discussed in the Data Analysis Section 4.6. The inclusion of the Month effect, $b_m$, would result in the following formula: 

$$Y_{h,a,f,m} = \hat{\mu} + b_h + b_a + b_f + b_m + \varepsilon_{h,a}$$
where $b_m$ refers to the Month effect or bias $m$

```{r echo=FALSE, include=FALSE}
## Matchup & GDPD & Month Model ##

# Model taking into account the Month effect, b_m
b_m <- train %>%
  left_join(b_h, by='HomeTeam') %>%
  left_join(b_a, by='AwayTeam') %>%
  left_join(b_f, by='GDPD') %>%
  group_by(Month) %>%
  summarize(b_m = mean(goal_diff - mu - b_h - b_a - b_f))

# predict all unknown goals with mu, b_h, b_a, b_f and b_m
predicted_goaldiff <- test %>% 
  left_join(b_h, by='HomeTeam') %>%
  left_join(b_a, by='AwayTeam') %>%
  left_join(b_f, by="GDPD") %>%
  left_join(b_m, by='Month') %>%
  mutate(pred = mu + b_h + b_a + b_f + b_m) %>%
  pull(pred)

# calculate RMSE of Matchup & GDPD & Month effect
rmse_matchupXGDPDXmontheffect <- RMSE(test$goal_diff, predicted_goaldiff)

# calculate Accuracy of Matchup & GDPD & Month effect
accuracy_matchupXGDPDXmontheffect <- mean(if_else(predicted_goaldiff > 0.5, "H", "NH") == test$FTR)

# Test and save rmse results 
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="[Test] MatchUp & GDPD & Month Effect Model",  
                                     RMSE = rmse_matchupXGDPDXmontheffect, 
                                     Accuracy = accuracy_matchupXGDPDXmontheffect))
# Consolidate results
rmse_results %>% knitr::kable()

```

The resulting RMSE of the Matchup & Form & Month Effect Model on the ```test``` dataset is **1.574** with an accuracy of predicting home win at **64.0%**. This represents an improvement in the RMSE of ~7.5% when compared with the Naive-Baseline Model. Comparing the RMSE with the actual matchday goal difference which spans from -6 to 14, this error is ~11.2%. However, when compared to the Matchup & Form Effect Model, both RMSE and accuracy are worse of. As a result, the previous model, the Matchup & Form Effect Model, will be selected for further optimization. 

## Matchup & Form Effect + Regularization

To mitigate the risk of overfitting, **regularization** is applied to the selected model. The use of regularization penalizes on matchups or Form Categories with low occurrences. As explained in Section 4.5, there are some Form Categories where there is only 1 data point. The tuning parameter, lambda, resulting in the smallest RMSE will be used to shrink the HomeTeam, AwayTeam and Form effect for the test set. The formula that represents the Movie & User Effect + Regularization Model is: 

$$\frac{1}{N} \sum_{h,a,f} (y_{h,a,f} - \mu - b_h - b_a - b_f)^{2} + \lambda (\sum_{h}b_{h}^2 + \sum_{a}b_{a}^2 + \sum_{f}b_{f}^2)$$   
where $\lambda$ is the tuning parameter applied to the movie and user effect. 

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4.5}
# Determining the lambda with the lowest RMSE
lambdas <- seq(from=80, to=140, by=0.5)

# output RMSE of each lambda, repeat earlier steps (with regularization)
rmses <- sapply(lambdas, function(l){
  # calculate average goal_diff across training data
  mu <- mean(train$goal_diff)
  # compute regularized HomeTeam bias term
  b_h <- train %>% 
    group_by(HomeTeam) %>%
    summarize(b_h = sum(goal_diff - mu)/(n()+l))
  # compute regularize AwayTeam bias term
  b_a <- train %>% 
    left_join(b_h, by="HomeTeam") %>%
    group_by(AwayTeam) %>%
    summarize(b_a = sum(goal_diff - b_h - mu)/(n()+l))
  # compute regularize GDPD bias term
  b_f <- train %>%
    left_join(b_h, by='HomeTeam') %>%
    left_join(b_a, by='AwayTeam') %>%
    group_by(GDPD) %>%
    summarize(b_f = sum(goal_diff - b_h - b_a - mu)/(n()+l))
  # compute predictions on test set based on these above terms
  predicted_goaldiff <- test %>% 
    left_join(b_h, by = "HomeTeam") %>%
    left_join(b_a, by = "AwayTeam") %>%
    left_join(b_f, by = "GDPD") %>%
    mutate(pred = mu + b_h + b_a + b_f) %>%
    pull(pred)
  # output RMSE of these predictions
  return(RMSE(predicted_goaldiff, test$goal_diff))
})
  
# quick plot of RMSE vs lambdas
qplot(lambdas, rmses,
      xlab = "Lambda",
      ylab = "RMSE",
      main = "Distribution - RMSE through Lambda")
```

From the plot, it can observed that the lambda value that corresponds to lowest RMSE of 1.552 in the train set is 105.5.

```{r echo=FALSE, include=FALSE}
# print minimum RMSE 
min(rmses)

# The linear model with the minimizing lambda
lam <- lambdas[which.min(rmses)]
lam

# compute regularize HomeTeam bias term
b_h <- train %>% 
  group_by(HomeTeam) %>%
  summarize(b_h = sum(goal_diff - mu)/(n()+lam))
# compute regularize AwayTeam bias term
b_a <- train %>% 
  left_join(b_h, by="HomeTeam") %>%
  group_by(AwayTeam) %>%
  summarize(b_a = sum(goal_diff - mu - b_h)/(n()+lam))
# compute regularize AwayTeam bias term
b_f <- train %>%
  left_join(b_h, by='HomeTeam') %>%
  left_join(b_a, by='AwayTeam') %>%
  group_by(GDPD) %>%
  summarize(b_f = sum(goal_diff - b_h - b_a - mu)/(n()+lam))
# compute predictions on test set based on these above terms
predicted_goaldiff <- test %>% 
  left_join(b_h, by = "HomeTeam") %>%
  left_join(b_a, by = "AwayTeam") %>%
  left_join(b_f, by = "GDPD") %>%
  mutate(pred = mu + b_h + b_a + b_f) %>%
  pull(pred)
# output RMSE of these predictions
rmse_regularizedXmatchupXGDPDeffect <- RMSE(predicted_goaldiff, test$goal_diff)

# calculate Accuracy of these predictions
accuracy_regularizeXmatchupXGDPDeffect <- mean(if_else(predicted_goaldiff > 0.5, "H", "NH") == test$FTR)

# Test and save RMSE results 
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="[Test] Matchup & Form Effect + Regularization Model",  
                                     RMSE = rmse_regularizedXmatchupXGDPDeffect,
                                     Accuracy = accuracy_regularizeXmatchupXGDPDeffect))

# Consolidate results
rmse_results %>% knitr::kable()
```

Applying this lambda value onto the the test set, the resulting RMSE (of the Matchup & Form Effect + Regularization Model) is **1.552** with an accuracy of predicting home win at **63.0%**. This represents an improvement in the RMSE of ~8.8% when compared with the Naive-Baseline Model. Comparing the RMSE with the actual matchday goal difference which spans from -6 to 14, this error is ~11.1%.

With the resulting RMSE for the Matchup & Form Effect + Regularization Model on the ```test``` dataset being the lowest at 1.552, the Matchup & Form Effect + Regularization Model will be selected as the final algorithm to be applied on the ```validation``` dataset.

```{r echo=FALSE, include=FALSE}

# compute regularize HomeTeam bias term
b_h <- model %>% 
  group_by(HomeTeam) %>%
  summarize(b_h = sum(goal_diff - mu)/(n()+lam))
# compute regularize AwayTeam bias term
b_a <- model %>% 
  left_join(b_h, by="HomeTeam") %>%
  group_by(AwayTeam) %>%
  summarize(b_a = sum(goal_diff - mu - b_h)/(n()+lam))
# compute regularize GDPD bias term
b_f <- model %>%
  left_join(b_h, by='HomeTeam') %>%
  left_join(b_a, by='AwayTeam') %>%
  group_by(GDPD) %>%
  summarize(b_f = sum(goal_diff - b_h - b_a - mu)/(n()+lam))
# compute predictions on test set based on these above terms
predicted_goaldiff <- validation %>% 
  left_join(b_h, by = "HomeTeam") %>%
  left_join(b_a, by = "AwayTeam") %>%
  left_join(b_f, by = "GDPD") %>%
  mutate(pred = mu + b_h + b_a + b_f) %>%
  pull(pred)
# output RMSE of these predictions
rmse_regularizedXmatchupXGDPDeffect <- RMSE(predicted_goaldiff, validation$goal_diff)

# calculate Accuracy of these predictions
accuracy_regularizeXmatchupXGDPDeffect <- mean(if_else(predicted_goaldiff > 0.5, "H", "NH") == validation$FTR)

# Test and save RMSE results 
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="[Validation] Matchup & Form Effect + Regularization Model",  
                                     RMSE = rmse_regularizedXmatchupXGDPDeffect,
                                     Accuracy = accuracy_regularizeXmatchupXGDPDeffect))
```

\begin{center} Table 5.1: RMSE Results for All Models \end{center}
```{r echo=FALSE}
# Consolidate results
rmse_results %>% knitr::kable()
```

As reflected in the table, the RMSE for the final Matchup & Form Effect + Regularization Model is **1.550** with an accuracy of predicting home win at **63.3%**. Comparing the RMSE with the actual matchday goal difference which spans from -6 to 14, this error is ~11.1%.

## Conclusion
Characterized by the lowest RMSE value, the Matchup & Form Effect + Regularization Model is regarded as the optimal model for predicting matchday goal differences. From the executive summary, it is noteworthy to point out that there could be other features like game statistics, formation deployed and the ability of individual players that could be applied to further improve on the model's prediction accuracy. However, due to the limitations of the data available online, these models cannot be validated. Overall, a RMSE of **1.550** with an accuracy of predicting home win at **63.3%** should be regarded an acceptable prediction model, considering the English Premier League to be the most unpredictable soccer leagues in the world. 